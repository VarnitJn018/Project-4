{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0393cfa8-b926-49c1-a8b1-9ce6307abc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-2.9.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (3.5.0)\n",
      "Collecting jiter<1,>=0.10.0\n",
      "  Downloading jiter-0.12.0-cp310-cp310-macosx_11_0_arm64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: tqdm>4 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: sniffio in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/michaelhuang/anaconda3/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "Successfully installed distro-1.9.0 jiter-0.12.0 openai-2.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecc9a42-cdd5-4ee9-b009-a133189ff2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Note: 'tiktoken' not found. Using simple character count for token limits.\n",
      "================================================================================\n",
      "Loading data from: Training data.xlsx\n",
      "\n",
      "ü§ñ Initializing LLM Assistant...\n",
      "LLM Error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************7gwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n",
      "\n",
      "Initializing Gaussian Process model...\n",
      "Fitting Gaussian Process model on 3 samples...\n",
      "‚úì GP model fitted.\n",
      "\n",
      "Generating 12 qLogNEI suggestions...\n",
      "‚úì Suggestions saved to: /Users/michaelhuang/Desktop/optimization_suggestions.xlsx\n",
      "\n",
      "ü§ñ LLM is reviewing suggestions...\n",
      "LLM Error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************7gwA. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}, 'status': 401}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- DEPENDENCY CHECKS ---\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    OPENAI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPENAI_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è 'openai' library not found. LLM Assistant will be DISABLED.\")\n",
    "\n",
    "try:\n",
    "    import tiktoken\n",
    "except ImportError:\n",
    "    tiktoken = None\n",
    "    print(\"‚ö†Ô∏è Note: 'tiktoken' not found. Using simple character count for token limits.\")\n",
    "\n",
    "# BoTorch imports\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_mll\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import qLogNoisyExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.transforms import normalize, unnormalize, standardize\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0. CONFIGURATION & PROMPTS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "PROMPTS = {\n",
    "    \"experiment_overview\": \"\"\"\n",
    "You are an expert scientist assisting with a chemical optimization experiment.\n",
    "Goal: Maximize [target].\n",
    "Description: [description]\n",
    "Parameters:\n",
    "[parameters_and_bounds]\n",
    "Constraint: [constraint]\n",
    "Domain: [domain]\n",
    "\n",
    "Please provide a brief overview of the experiment and the challenges involved.\n",
    "\"\"\",\n",
    "    \"starter\": \"\"\"\n",
    "Based on the experiment description, generate [n_hypotheses] initial hypotheses for points that might maximize the target.\n",
    "Consider the chemical properties and potential interactions.\n",
    "Return the response in JSON format with fields: \"comment\" (string) and \"hypotheses\" (list of dicts with \"name\", \"rationale\", \"confidence\", \"points\").\n",
    "Each \"point\" must be a dictionary of parameter names and values.\n",
    "\"\"\",\n",
    "    \"comment_selection\": \"\"\"\n",
    "Current Iteration: [iteration]\n",
    "We have trained a Gaussian Process model and generated candidate points using qLogNEI (Log Noisy Expected Improvement).\n",
    "\n",
    "Top Suggestions:\n",
    "[suggestions]\n",
    "\n",
    "Historical Data (Top/Recent):\n",
    "[dataset]\n",
    "\n",
    "Please analyze these suggestions. Which ones seem most chemically promising based on the historical data and your scientific knowledge?\n",
    "Select the best ones or suggest modifications if they violate \"chemical sense\" (though strictly respect the bounds).\n",
    "Return JSON with \"comment\" and \"hypotheses\" (where \"points\" are the selected/refined candidates).\n",
    "\"\"\",\n",
    "    \"conclusion\": \"\"\"\n",
    "The optimization batch is complete.\n",
    "Data Summary:\n",
    "[dataset]\n",
    "\n",
    "Provide a final conclusion on the findings and recommended next steps.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. MOCK BORA CLASSES (To satisfy Assistant dependencies)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class MockParameter:\n",
    "    def __init__(self, name, bounds, step=None):\n",
    "        self.name = name\n",
    "        self.bounds = bounds\n",
    "        self.description = \"Chemical parameter\"\n",
    "        self.type = \"continuous\"\n",
    "        self.step = step\n",
    "        self.categories = []\n",
    "\n",
    "    def get_bounds(self):\n",
    "        return self.bounds\n",
    "\n",
    "    def is_valid_value(self, v):\n",
    "        return self.bounds[0] <= v <= self.bounds[1]\n",
    "\n",
    "class MockTarget:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "class MockConstraint:\n",
    "    def __init__(self, description):\n",
    "        self.description = description\n",
    "        self.constraint = None \n",
    "    def eval(self, **kwargs): return 0\n",
    "    def allowed(self, val): return [True]\n",
    "\n",
    "class MockExperiment:\n",
    "    def __init__(self, name, parameters, target_name):\n",
    "        self.name = name\n",
    "        self.parameters = parameters\n",
    "        self.dim = len(parameters)\n",
    "        self.type = \"continuous\"\n",
    "        self.constraint = MockConstraint(\"Sum of components (excluding P10) <= 5\")\n",
    "        self.domain = \"Chemical Formulation\"\n",
    "        self.target = MockTarget(target_name)\n",
    "        self.default_precision = 4\n",
    "        self.keys = [p.name for p in parameters]\n",
    "        self.pbounds = {p.name: p.bounds for p in parameters}\n",
    "        self.description = \"Optimize a chemical formula to maximize the target yield.\"\n",
    "    \n",
    "    def get_parameter(self, name):\n",
    "        for p in self.parameters:\n",
    "            if p.name == name: return p\n",
    "        return None\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. ASSISTANT CLASS (Adapted)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class CommentType(Enum):\n",
    "    PREOPTIMIZATION = 0\n",
    "    POINTS = 1\n",
    "    CONCLUSION = 3\n",
    "\n",
    "class BaseComment:\n",
    "    def __init__(self, response: str, iteration: int) -> None:\n",
    "        self._iteration = iteration\n",
    "        self._comment = response\n",
    "        self.type = None\n",
    "        self.hypotheses = []\n",
    "\n",
    "    def __str__(self): return str(self._comment)\n",
    "    @property\n",
    "    def comment(self): return self._comment\n",
    "    @property\n",
    "    def iteration(self): return self._iteration\n",
    "    @property\n",
    "    def is_valid(self): return self._comment is not None\n",
    "\n",
    "class Comment(BaseComment):\n",
    "    def __init__(self, response: str, iteration: int, experiment, llm_model, api_key):\n",
    "        super().__init__(response, iteration)\n",
    "        self._experiment = experiment\n",
    "        self._comment = \"\"\n",
    "        self._hypotheses = []\n",
    "        \n",
    "        try:\n",
    "            # Parse JSON\n",
    "            json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                self._comment = data.get(\"comment\", \"\")\n",
    "                self._hypotheses = data.get(\"hypotheses\", [])\n",
    "        except:\n",
    "            self._comment = response # Fallback if not valid JSON\n",
    "\n",
    "    def __str__(self):\n",
    "        return json.dumps({\"comment\": self._comment, \"hypotheses\": self._hypotheses}, indent=2)\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, api_key, experiment, log_path, llm_model=\"gpt-4o-mini\"):\n",
    "        self._api_key = api_key\n",
    "        self._experiment = experiment\n",
    "        self._log_path = log_path\n",
    "        self._model = llm_model\n",
    "        \n",
    "        if OPENAI_AVAILABLE:\n",
    "            self._client = OpenAI(api_key=api_key)\n",
    "        else:\n",
    "            self._client = None\n",
    "            \n",
    "        self._comments = []\n",
    "        self._chat_history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are BORA, an AI scientist for Bayesian Optimization.\"}\n",
    "        ]\n",
    "        \n",
    "        # Initialize encoder if tiktoken is available\n",
    "        self._encoding = None\n",
    "        if tiktoken:\n",
    "            try:\n",
    "                self._encoding = tiktoken.encoding_for_model(llm_model)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def _get_token_count(self, text):\n",
    "        if self._encoding:\n",
    "            return len(self._encoding.encode(text))\n",
    "        else:\n",
    "            return len(text) // 4\n",
    "\n",
    "    def _chat_completion(self, messages):\n",
    "        if not self._client: return None\n",
    "        try:\n",
    "            completion = self._client.chat.completions.create(\n",
    "                model=self._model, messages=messages, temperature=0.7\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"LLM Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _fill_prompt(self, template, extra_subs={}):\n",
    "        txt = template\n",
    "        txt = txt.replace(\"[target]\", self._experiment.target.name)\n",
    "        txt = txt.replace(\"[description]\", self._experiment.description)\n",
    "        txt = txt.replace(\"[constraint]\", self._experiment.constraint.description)\n",
    "        txt = txt.replace(\"[domain]\", self._experiment.domain)\n",
    "        \n",
    "        # Parameters\n",
    "        p_str = \"\"\n",
    "        for p in self._experiment.parameters:\n",
    "            p_str += f\"- {p.name}: bounds={p.get_bounds()}\\n\"\n",
    "        txt = txt.replace(\"[parameters_and_bounds]\", p_str)\n",
    "        \n",
    "        for k, v in extra_subs.items():\n",
    "            txt = txt.replace(f\"[{k}]\", str(v))\n",
    "        return txt\n",
    "\n",
    "    def pre_optimization_comment(self, n_hypotheses=3):\n",
    "        if not OPENAI_AVAILABLE: return None\n",
    "        \n",
    "        prompt = self._fill_prompt(PROMPTS[\"starter\"], {\"n_hypotheses\": n_hypotheses})\n",
    "        self._chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = self._chat_completion(self._chat_history)\n",
    "        if resp:\n",
    "            comment = Comment(resp, 0, self._experiment, self._model, self._api_key)\n",
    "            comment.type = CommentType.PREOPTIMIZATION\n",
    "            self._comments.append(comment)\n",
    "            self._chat_history.append({\"role\": \"assistant\", \"content\": resp})\n",
    "            self._log_comment(comment)\n",
    "            return comment\n",
    "        return None\n",
    "\n",
    "    def comment_and_select_point(self, data: pd.DataFrame, suggestions: pd.DataFrame):\n",
    "        if not OPENAI_AVAILABLE: \n",
    "            print(\"‚ö†Ô∏è OpenAI library missing. Skipping LLM review.\")\n",
    "            return None\n",
    "            \n",
    "        data_summary = data.tail(10).to_string(index=False)\n",
    "        sugg_str = suggestions.to_string(index=False)\n",
    "        \n",
    "        prompt = self._fill_prompt(PROMPTS[\"comment_selection\"], {\n",
    "            \"iteration\": len(data),\n",
    "            \"dataset\": data_summary,\n",
    "            \"suggestions\": sugg_str\n",
    "        })\n",
    "        \n",
    "        # Check context window (simple check)\n",
    "        if self._get_token_count(prompt) > 100000:\n",
    "            print(\"‚ö†Ô∏è Prompt too long, truncating history...\")\n",
    "            data_summary = data.tail(5).to_string(index=False)\n",
    "            prompt = self._fill_prompt(PROMPTS[\"comment_selection\"], {\n",
    "                \"iteration\": len(data),\n",
    "                \"dataset\": data_summary,\n",
    "                \"suggestions\": sugg_str\n",
    "            })\n",
    "\n",
    "        self._chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        \n",
    "        resp = self._chat_completion(self._chat_history)\n",
    "        if resp:\n",
    "            comment = Comment(resp, len(data), self._experiment, self._model, self._api_key)\n",
    "            comment.type = CommentType.POINTS\n",
    "            self._comments.append(comment)\n",
    "            self._chat_history.append({\"role\": \"assistant\", \"content\": resp})\n",
    "            self._log_comment(comment)\n",
    "            return comment\n",
    "        return None\n",
    "\n",
    "    def _log_comment(self, comment):\n",
    "        with open(self._log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n\\n## Iteration {comment.iteration}\\n{comment}\\n\")\n",
    "            print(f\"\\n--- LLM Comment ---\\n{comment.comment}\\n-------------------\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. BOTORCH OPTIMIZER (Gaussian Process + qLogNEI)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class BoTorchOptimizer:\n",
    "    \"\"\"BoTorch-based optimizer using Gaussian Process and qLogNEI\"\"\"\n",
    "    def __init__(self, bounds: np.ndarray, random_state: int = 42):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.dtype = torch.double\n",
    "        self.bounds_np = bounds\n",
    "        self.bounds_tensor = torch.tensor(bounds.T, device=self.device, dtype=self.dtype)\n",
    "        torch.manual_seed(random_state)\n",
    "        self.model = None\n",
    "        self.train_y_mean = None\n",
    "        self.train_y_std = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        if np.isnan(X).any() or np.isnan(y).any():\n",
    "            raise ValueError(\"Input X or y contains NaNs!\")\n",
    "\n",
    "        self.X_train = torch.tensor(X, device=self.device, dtype=self.dtype)\n",
    "        self.y_train = torch.tensor(y.reshape(-1, 1), device=self.device, dtype=self.dtype)\n",
    "        \n",
    "        self.train_y_mean = self.y_train.mean()\n",
    "        self.train_y_std = self.y_train.std()\n",
    "        if self.train_y_std < 1e-9:\n",
    "             self.train_y_std = torch.tensor(1.0, device=self.device, dtype=self.dtype)\n",
    "\n",
    "        self.train_X_norm = normalize(self.X_train, self.bounds_tensor)\n",
    "        self.train_y_std_norm = standardize(self.y_train)\n",
    "        \n",
    "        print(f\"Fitting Gaussian Process model on {len(X)} samples...\")\n",
    "        self.model = SingleTaskGP(self.train_X_norm, self.train_y_std_norm)\n",
    "        mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
    "        fit_gpytorch_mll(mll)\n",
    "        print(f\"‚úì GP model fitted.\")\n",
    "\n",
    "    def suggest_batch_qlognei(self, q: int = 10):\n",
    "        \"\"\"Use qLogNoisyExpectedImprovement with GP\"\"\"\n",
    "        \n",
    "        acq_func = qLogNoisyExpectedImprovement(\n",
    "            model=self.model,\n",
    "            X_baseline=self.train_X_norm,\n",
    "        )\n",
    "        \n",
    "        # Constraints definition\n",
    "        idx_p10 = 5\n",
    "        idx_others = [i for i in range(self.bounds_tensor.shape[1]) if i != idx_p10]\n",
    "        lower = self.bounds_tensor[0]\n",
    "        upper = self.bounds_tensor[1]\n",
    "        ranges = upper - lower\n",
    "        sum_lower_others = lower[idx_others].sum()\n",
    "        rhs_val = (sum_lower_others - 5.0).item()\n",
    "        coeffs_tensor = -ranges[idx_others]\n",
    "        indices_tensor = torch.tensor(idx_others, device=self.device, dtype=torch.long)\n",
    "        \n",
    "        inequality_constraints = [(indices_tensor, coeffs_tensor, rhs_val)]\n",
    "\n",
    "        try:\n",
    "            candidates_norm, _ = optimize_acqf(\n",
    "                acq_function=acq_func, \n",
    "                bounds=torch.stack([torch.zeros_like(lower), torch.ones_like(upper)]),\n",
    "                q=q, \n",
    "                num_restarts=10, \n",
    "                raw_samples=512, \n",
    "                inequality_constraints=inequality_constraints, \n",
    "                sequential=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "             print(f\"Optimization warning: {e}. Falling back to unconstrained initialization.\")\n",
    "             candidates_norm, _ = optimize_acqf(\n",
    "                acq_function=acq_func, \n",
    "                bounds=torch.stack([torch.zeros_like(lower), torch.ones_like(upper)]),\n",
    "                q=q, \n",
    "                num_restarts=10, \n",
    "                raw_samples=512, \n",
    "                sequential=True\n",
    "            )\n",
    "\n",
    "        candidates_raw = unnormalize(candidates_norm, self.bounds_tensor).detach().cpu().numpy()\n",
    "        \n",
    "        # Post-process\n",
    "        final_candidates = []\n",
    "        for cand in candidates_raw:\n",
    "            cand = self._apply_rounding_and_constraints(cand, idx_p10, idx_others)\n",
    "            final_candidates.append(cand)\n",
    "        return np.array(final_candidates)\n",
    "\n",
    "    def _apply_rounding_and_constraints(self, candidate, idx_p10, idx_others):\n",
    "        cand = candidate.copy()\n",
    "        p10_val = round(cand[idx_p10] / 0.2) * 0.2\n",
    "        cand[idx_p10] = max(1.2, p10_val)\n",
    "        cand[idx_others] = np.round(cand[idx_others] / 0.25) * 0.25\n",
    "        cand[idx_others] = np.maximum(0.0, cand[idx_others])\n",
    "        while cand[idx_others].sum() > 5.0 + 1e-6:\n",
    "            local_indices = np.argsort(cand[idx_others])[::-1]\n",
    "            reduced = False\n",
    "            for loc_i in local_indices:\n",
    "                real_idx = idx_others[loc_i]\n",
    "                if cand[real_idx] >= 0.25:\n",
    "                    cand[real_idx] -= 0.25\n",
    "                    reduced = True\n",
    "                    break\n",
    "            if not reduced: break\n",
    "        return cand\n",
    "\n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"Predict using the GP Posterior\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, device=self.device, dtype=self.dtype)\n",
    "            X_norm = normalize(X_tensor, self.bounds_tensor)\n",
    "            \n",
    "            posterior = self.model.posterior(X_norm)\n",
    "            mu = posterior.mean * self.train_y_std + self.train_y_mean\n",
    "            sigma = posterior.variance.sqrt() * self.train_y_std\n",
    "            \n",
    "            return mu.cpu().numpy().ravel(), sigma.cpu().numpy().ravel()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. VISUALIZATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def create_enhanced_visualization(suggestions, X, y, feature_cols, output_dir, model_wrapper):\n",
    "    best_sugg = max(suggestions, key=lambda x: x['predicted_target'])\n",
    "    best_params = best_sugg['params']\n",
    "    n_features = len(feature_cols)\n",
    "    cols_per_row = 5\n",
    "    rows_needed = (n_features + cols_per_row - 1) // cols_per_row\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 5 * rows_needed + 5))\n",
    "    gs = gridspec.GridSpec(rows_needed + 1, cols_per_row, height_ratios=[1]*rows_needed + [0.8], hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    for i, col_name in enumerate(feature_cols):\n",
    "        row = i // cols_per_row\n",
    "        col = i % cols_per_row\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        \n",
    "        x_min, x_max = model_wrapper.bounds_np[i]\n",
    "        if x_min == x_max: view_min, view_max = x_min - 0.1, x_max + 0.1\n",
    "        else: view_min, view_max = x_min - (x_max - x_min)*0.05, x_max + (x_max - x_min)*0.05\n",
    "        \n",
    "        x_grid = np.linspace(view_min, view_max, 100)\n",
    "        X_visualize = np.tile(best_params, (100, 1))\n",
    "        X_visualize[:, i] = x_grid\n",
    "        y_pred_grid, y_std_grid = model_wrapper.predict(X_visualize)\n",
    "        \n",
    "        lower_bound = np.maximum(y_pred_grid - 1.96 * y_std_grid, 0)\n",
    "        upper_bound = np.maximum(y_pred_grid + 1.96 * y_std_grid, 0)\n",
    "        \n",
    "        ax.plot(x_grid, np.maximum(y_pred_grid, 0), color='#2c3e50', linewidth=2.5, label='GP Mean')\n",
    "        ax.fill_between(x_grid, lower_bound, upper_bound, color='#3498db', alpha=0.15, label='95% CI')\n",
    "        ax.scatter(X[:, i], y, color='gray', alpha=0.4, s=30)\n",
    "        ax.scatter([best_params[i]], [best_sugg['predicted_target']], color='#e74c3c', s=150, marker='*', zorder=10, edgecolor='black')\n",
    "        ax.set_xlabel(col_name, fontsize=10, fontweight='bold')\n",
    "        if col == 0: ax.set_ylabel(\"Predicted Target\", fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, linestyle='--')\n",
    "        ax.legend(loc='best', fontsize=7, framealpha=0.8)\n",
    "\n",
    "    ax_bar = fig.add_subplot(gs[rows_needed, :])\n",
    "    top_n = sorted(suggestions, key=lambda x: x['predicted_target'], reverse=True)[:10]\n",
    "    labels = [f\"Best Hist.\\n({y.max():.3f})\"] + [f\"#{k+1}\" for k in range(len(top_n))]\n",
    "    values = [y.max()] + [s['predicted_target'] for s in top_n]\n",
    "    bar_colors = ['#34495e'] + ['#e74c3c' for _ in top_n]\n",
    "    \n",
    "    bars = ax_bar.bar(range(len(labels)), values, capsize=5, color=bar_colors, alpha=0.85, edgecolor='black')\n",
    "    ax_bar.set_xticks(range(len(labels)))\n",
    "    ax_bar.set_xticklabels(labels, fontsize=10)\n",
    "    ax_bar.set_ylabel('Target Value', fontsize=11, fontweight='bold')\n",
    "    ax_bar.set_title('Top Suggestions (qLogNEI)', fontsize=13)\n",
    "    \n",
    "    legend_elements = [Patch(facecolor='#34495e', edgecolor='black', label='Best Historical'),\n",
    "                       Patch(facecolor='#e74c3c', edgecolor='black', label='qLogNEI Suggestion')]\n",
    "    ax_bar.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    plot_path = os.path.join(output_dir, 'optimization_landscape.png')\n",
    "    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. MAIN LOOP (With LLM Assistant)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def optimize_dataset(filepath: str, n_suggestions: int = 12, output_dir: str = None):\n",
    "    if output_dir is None: output_dir = os.path.dirname(os.path.abspath(filepath))\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"Loading data from:\", filepath)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(filepath) if filepath.endswith('.xlsx') else pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    feature_cols = [\n",
    "        'AcidRed871_0gL', 'L-Cysteine-50gL', 'MethyleneB_250mgL', 'NaCl-3M', \n",
    "        'NaOH-1M', 'P10-MIX1', 'PVP-1wt', 'RhodamineB1_0gL', 'SDS-1wt', 'Sodiumsilicate-1wt'\n",
    "    ]\n",
    "    target_col = 'Target'\n",
    "    \n",
    "    df_clean = df[feature_cols + [target_col]].copy()\n",
    "    for col in df_clean.columns: df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    df_clean.dropna(inplace=True)\n",
    "    \n",
    "    X = df_clean[feature_cols].values\n",
    "    y = df_clean[target_col].values\n",
    "    \n",
    "    # --- SETUP LLM ASSISTANT ---\n",
    "    mock_params = []\n",
    "    # Index 5 is P10-MIX1 (step 0.2), others step 0.25\n",
    "    for i, name in enumerate(feature_cols):\n",
    "        min_b = max(0, X[:, i].min() * 0.9)\n",
    "        max_b = max(X[:, i].max() * 1.1, 1e-6)\n",
    "        if i == 5: # P10\n",
    "            min_b = 1.0001\n",
    "            max_b = max(max_b, 2.0)\n",
    "            step = 0.2\n",
    "        else:\n",
    "            step = 0.25\n",
    "        mock_params.append(MockParameter(name, [min_b, max_b], step))\n",
    "    \n",
    "    mock_exp = MockExperiment(\"Chemical Opt\", mock_params, target_col)\n",
    "    log_file = os.path.join(output_dir, \"assistant_log.md\")\n",
    "    \n",
    "    print(\"\\nü§ñ Initializing LLM Assistant...\")\n",
    "    assistant = Assistant(OPENAI_API_KEY, mock_exp, log_file)\n",
    "    assistant.pre_optimization_comment()\n",
    "    \n",
    "    # --- RUN OPTIMIZATION ---\n",
    "    print(\"\\nInitializing Gaussian Process model...\")\n",
    "    bounds = np.array([p.bounds for p in mock_params])\n",
    "    botorch_opt = BoTorchOptimizer(bounds=bounds)\n",
    "    botorch_opt.fit(X, y)\n",
    "    \n",
    "    print(f\"\\nGenerating {n_suggestions} qLogNEI suggestions...\")\n",
    "    candidates = botorch_opt.suggest_batch_qlognei(q=n_suggestions)\n",
    "    \n",
    "    all_suggestions = []\n",
    "    for cand in candidates:\n",
    "        mu, std = botorch_opt.predict(cand.reshape(1, -1))\n",
    "        all_suggestions.append({\n",
    "            'method': 'qLogNEI', \n",
    "            'params': cand, \n",
    "            'predicted_target': float(mu[0]), \n",
    "            'uncertainty': float(std[0])\n",
    "        })\n",
    "\n",
    "    suggestions_df = pd.DataFrame([\n",
    "        {'Method': s['method'], \n",
    "         **{feature_cols[i]: s['params'][i] for i in range(len(feature_cols))},\n",
    "         'Predicted Target': s['predicted_target'], \n",
    "         'Uncertainty': s['uncertainty']} \n",
    "        for s in all_suggestions\n",
    "    ])\n",
    "    \n",
    "    out_file = os.path.join(output_dir, 'optimization_suggestions.xlsx')\n",
    "    suggestions_df.to_excel(out_file, index=False)\n",
    "    print(f\"‚úì Suggestions saved to: {out_file}\")\n",
    "    \n",
    "    # --- LLM REVIEW ---\n",
    "    print(\"\\nü§ñ LLM is reviewing suggestions...\")\n",
    "    assistant.comment_and_select_point(df_clean, suggestions_df)\n",
    "\n",
    "    create_enhanced_visualization(all_suggestions, X, y, feature_cols, output_dir, botorch_opt)\n",
    "    return suggestions_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "    input_file = \"Training data.xlsx\"\n",
    "    \n",
    "    if os.path.exists(input_file):\n",
    "        optimize_dataset(filepath=input_file, n_suggestions=12, output_dir=desktop_path)\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find '{input_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a43e41b-95a1-4344-9dbf-1251607b8013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading data from: HER_virtual_data.xlsx\n",
      "Data: 812 valid experiments | Best Target: 27.7041\n",
      "\n",
      "Initializing qUCB model...\n",
      "‚úì BoTorch GP model fitted on 812 samples\n",
      "\n",
      "================================================================================\n",
      "GENERATING SUGGESTIONS (qUCB ONLY)\n",
      "Constraints: Sum(others) <= 5 | P10 > 1 | Discrete Steps\n",
      "================================================================================\n",
      "\n",
      "‚úì Suggestions saved to: /Users/michaelhuang/Desktop/optimization_suggestions.xlsx\n",
      "‚úì Visualization saved to: /Users/michaelhuang/Desktop/optimization_landscape.png\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Auto-detect Desktop path\n",
    "    desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n",
    "    input_file = \"HER_virtual_data.xlsx\"\n",
    "    \n",
    "    if os.path.exists(input_file):\n",
    "        optimize_dataset(\n",
    "            filepath=input_file, \n",
    "            n_suggestions=12,\n",
    "            output_dir=desktop_path \n",
    "        )\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find '{input_file}'. Please check the filename.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b4f25-fee3-432a-b49c-bdb0bbf700ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
